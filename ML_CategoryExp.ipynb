{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be03f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import silhouette_score\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "698ae504",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "403 Client Error: Forbidden for url: http://127.0.0.1:5000/api/v1.0/poly_regress/actual_vs_predicted/2014/2028",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fetch the data from the API\u001b[39;00m\n\u001b[1;32m      5\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(api_endpoint)\n\u001b[0;32m----> 6\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# This will raise an exception for HTTP errors if any occur\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convert the JSON data to a DataFrame\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming the JSON data includes a 'Year' field in each record\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: http://127.0.0.1:5000/api/v1.0/poly_regress/actual_vs_predicted/2014/2028"
     ]
    }
   ],
   "source": [
    "# Single API endpoint providing all years' budget data\n",
    "api_endpoint = 'http://127.0.0.1:5000/api/v1.0/poly_regress/actual_vs_predicted/2014/2028'\n",
    "\n",
    "# Fetch the data from the API\n",
    "response = requests.get(api_endpoint)\n",
    "response.raise_for_status()  # This will raise an exception for HTTP errors if any occur\n",
    "data = response.json()\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "# Assuming the JSON data includes a 'Year' field in each record\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the first few rows to verify data structure\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a0e2e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select relevant numeric features for clustering\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# You may need to exclude non-relevant columns such as IDs or descriptive text\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])  \u001b[38;5;66;03m# This selects only numeric columns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Handle missing values\u001b[39;00m\n\u001b[1;32m      6\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace missing values with the mean of each column\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Select relevant numeric features for clustering\n",
    "# You may need to exclude non-relevant columns such as IDs or descriptive text\n",
    "features = df.select_dtypes(include=[np.number])  # This selects only numeric columns\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')  # Replace missing values with the mean of each column\n",
    "features = imputer.fit_transform(features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fcbfb75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Perform the clustering\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cluster \u001b[38;5;241m=\u001b[39m AgglomerativeClustering(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, distance_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, affinity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m, linkage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m cluster\u001b[38;5;241m.\u001b[39mfit_predict(features_scaled)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Optionally, plot a dendrogram to visualize the hierarchical clustering\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform the clustering\n",
    "cluster = AgglomerativeClustering(n_clusters=None, distance_threshold=0, affinity='euclidean', linkage='ward')\n",
    "cluster_labels = cluster.fit_predict(features_scaled)\n",
    "\n",
    "# Optionally, plot a dendrogram to visualize the hierarchical clustering\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title(\"Dendrogram\")\n",
    "dendrogram = sch.dendrogram(sch.linkage(features_scaled, method='ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e694c1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming 'features_scaled' is your dataset ready for clustering\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m features_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m max_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(num_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Ensuring we do not attempt too many clusters\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_clusters \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, max_clusters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):  \u001b[38;5;66;03m# Ensure at least two clusters\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming 'features_scaled' is your dataset ready for clustering\n",
    "import pickle\n",
    "\n",
    "num_samples = features_scaled.shape[0]\n",
    "max_clusters = min(num_samples - 1, 10)  # Ensuring we do not attempt too many clusters\n",
    "\n",
    "for n_clusters in range(2, max_clusters + 1):  # Ensure at least two clusters\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    cluster_labels = clusterer.fit_predict(features_scaled)\n",
    "    \n",
    "    filename = f\"cluster_models/cluster_model_Cluster{n_clusters}\"\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(clusterer, file)\n",
    "\n",
    "    # Calculate and print the silhouette score\n",
    "    silhouette_avg = silhouette_score(features_scaled, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "492f1d85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the final model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m final_cluster \u001b[38;5;241m=\u001b[39m AgglomerativeClustering(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, linkage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m final_labels \u001b[38;5;241m=\u001b[39m final_cluster\u001b[38;5;241m.\u001b[39mfit_predict(features_scaled)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Add cluster labels to the original DataFrame\u001b[39;00m\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m final_labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit the final model\n",
    "final_cluster = AgglomerativeClustering(n_clusters=5, linkage='ward')\n",
    "final_labels = final_cluster.fit_predict(features_scaled)\n",
    "\n",
    "# Add cluster labels to the original DataFrame\n",
    "df['Cluster'] = final_labels\n",
    "\n",
    "# Analyze clusters\n",
    "for i in range(final_cluster.n_clusters):\n",
    "    cluster_data = df[df['Cluster'] == i]\n",
    "    print(f\"Cluster {i} Summary:\")\n",
    "    print(cluster_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13cbd32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Applying PCA to reduce dimensions to 2 for visualization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Correct parameter for specifying number of principal components\u001b[39;00m\n\u001b[1;32m      3\u001b[0m principal_components \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(features_scaled)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Creating a DataFrame with PCA results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "# Applying PCA to reduce dimensions to 2 for visualization\n",
    "pca = PCA(n_components=2)  # Correct parameter for specifying number of principal components\n",
    "principal_components = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Creating a DataFrame with PCA results\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "pca_df['Cluster'] = final_labels  # Assuming 'final_labels' from your clustering results\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['r', 'g', 'b', 'y', 'c', 'm', 'k', 'orange', 'purple', 'brown']  # Extend color list if more than 10 clusters\n",
    "\n",
    "for i in range(final_cluster.n_clusters):  # Assuming 'final_cluster' from your clustering results\n",
    "    cluster_subset = pca_df[pca_df['Cluster'] == i]\n",
    "    plt.scatter(cluster_subset['Principal Component 1'], cluster_subset['Principal Component 2'], s=100, c=colors[i], label=f'Cluster {i}')\n",
    "\n",
    "plt.title('Clusters of Expenditure (PCA reduced)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "625d0931",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year, color \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(years, colors):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Select the column for the current year and drop NA values\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     current_year_data \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpense \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(millions)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_year_data\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data available for year \u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Skipping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "years = ['2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', '#FF5733', '#C70039', '#900C3F']\n",
    "\n",
    "# Ensure data is scaled properly for each year\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a figure outside the loop to plot all years on the same graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for year, color in zip(years, colors):\n",
    "    # Select the column for the current year and drop NA values\n",
    "    current_year_data = df[f'Expense {year}(millions)'].dropna().values.reshape(-1, 1)\n",
    "    if current_year_data.size == 0:\n",
    "        print(f\"No data available for year {year}. Skipping...\")\n",
    "        continue  # Skip this year if there is no data\n",
    "    \n",
    "    current_year_data_scaled = scaler.fit_transform(current_year_data)\n",
    "\n",
    "    # Dynamically adjust the range of k based on the number of samples\n",
    "    num_samples = len(current_year_data_scaled)\n",
    "    max_k = min(num_samples, 10)  # Maximum number of clusters is the lesser of num_samples or 10\n",
    "\n",
    "    inertias = []\n",
    "    k_values = range(1, max_k + 1)  # Adjust range to ensure k does not exceed number of samples\n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(current_year_data_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    # Plotting the inertia graph for each year on the same plot\n",
    "    plt.plot(k_values, inertias, marker='o', color=color, label=f'Year {year}')\n",
    "\n",
    "# Add labels, title, and legend to the plot\n",
    "plt.title('Elbow Method Across Different Years')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5558d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
